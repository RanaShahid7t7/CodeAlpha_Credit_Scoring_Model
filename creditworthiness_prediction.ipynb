{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creditworthiness Prediction\n",
        "\n",
        "This notebook predicts an individual's creditworthiness using past financial data through various classification algorithms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Explore Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic dataset for demonstration\n",
        "# In practice, you would load your actual dataset here\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "data = {\n",
        "    'age': np.random.randint(18, 70, n_samples),\n",
        "    'income': np.random.normal(50000, 20000, n_samples).clip(20000, 150000),\n",
        "    'debt_amount': np.random.normal(15000, 8000, n_samples).clip(0, 50000),\n",
        "    'credit_history_length': np.random.randint(1, 20, n_samples),\n",
        "    'num_credit_cards': np.random.randint(1, 8, n_samples),\n",
        "    'num_loans': np.random.randint(0, 5, n_samples),\n",
        "    'payment_history_score': np.random.uniform(0, 100, n_samples),  # 0-100 score\n",
        "    'late_payments_count': np.random.poisson(2, n_samples),\n",
        "    'utilization_ratio': np.random.uniform(0, 1, n_samples),  # Credit utilization\n",
        "    'employment_years': np.random.uniform(0, 30, n_samples)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create target variable based on financial indicators\n",
        "# Creditworthy (1) if: good payment history, low debt-to-income, reasonable utilization\n",
        "debt_to_income = df['debt_amount'] / (df['income'] + 1)\n",
        "df['creditworthy'] = (\n",
        "    (df['payment_history_score'] > 70) & \n",
        "    (debt_to_income < 0.4) & \n",
        "    (df['utilization_ratio'] < 0.7) &\n",
        "    (df['late_payments_count'] < 3)\n",
        ").astype(int)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nCreditworthy distribution:\")\n",
        "print(df['creditworthy'].value_counts())\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Check data types\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create engineered features\n",
        "df['debt_to_income_ratio'] = df['debt_amount'] / (df['income'] + 1)\n",
        "df['income_per_year'] = df['income'] / (df['age'] - 17 + 1)  # Approximate income per year of age\n",
        "df['avg_debt_per_loan'] = df['debt_amount'] / (df['num_loans'] + 1)\n",
        "df['credit_age_ratio'] = df['credit_history_length'] / (df['age'] - 17 + 1)\n",
        "df['payment_reliability'] = 100 - (df['late_payments_count'] * 10).clip(0, 100)\n",
        "df['total_credit_lines'] = df['num_credit_cards'] + df['num_loans']\n",
        "\n",
        "# Create interaction features\n",
        "df['income_utilization'] = df['income'] * (1 - df['utilization_ratio'])\n",
        "df['history_payment_score'] = df['credit_history_length'] * df['payment_history_score'] / 100\n",
        "\n",
        "# Binning features\n",
        "df['age_group'] = pd.cut(df['age'], bins=[0, 30, 45, 60, 100], labels=['Young', 'Middle', 'Senior', 'Elder'])\n",
        "df['income_group'] = pd.cut(df['income'], bins=[0, 40000, 70000, 100000, 200000], \n",
        "                            labels=['Low', 'Medium', 'High', 'Very High'])\n",
        "\n",
        "print(\"Feature engineering completed!\")\n",
        "print(f\"Total features after engineering: {df.shape[1]}\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize feature distributions\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "features_to_plot = ['income', 'debt_amount', 'payment_history_score', \n",
        "                    'debt_to_income_ratio', 'utilization_ratio', 'credit_history_length']\n",
        "\n",
        "for idx, feature in enumerate(features_to_plot):\n",
        "    axes[idx].hist(df[feature], bins=30, alpha=0.7, edgecolor='black')\n",
        "    axes[idx].set_title(f'Distribution of {feature}')\n",
        "    axes[idx].set_xlabel(feature)\n",
        "    axes[idx].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation heatmap\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "correlation_matrix = df[numeric_cols].corr()\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Feature Correlation Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select features for modeling\n",
        "# Exclude target variable and categorical features that need encoding\n",
        "feature_cols = [\n",
        "    'age', 'income', 'debt_amount', 'credit_history_length',\n",
        "    'num_credit_cards', 'num_loans', 'payment_history_score',\n",
        "    'late_payments_count', 'utilization_ratio', 'employment_years',\n",
        "    'debt_to_income_ratio', 'income_per_year', 'avg_debt_per_loan',\n",
        "    'credit_age_ratio', 'payment_reliability', 'total_credit_lines',\n",
        "    'income_utilization', 'history_payment_score'\n",
        "]\n",
        "\n",
        "X = df[feature_cols].copy()\n",
        "y = df['creditworthy'].copy()\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target distribution:\\n{y.value_counts()}\")\n",
        "print(f\"\\nTarget distribution (%):\\n{y.value_counts(normalize=True) * 100}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle any infinite or NaN values\n",
        "X = X.replace([np.inf, -np.inf], np.nan)\n",
        "X = X.fillna(X.mean())\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n",
        "print(f\"\\nTraining target distribution:\\n{y_train.value_counts()}\")\n",
        "print(f\"\\nTest target distribution:\\n{y_test.value_counts()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale features (important for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrame for better readability\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "print(\"Feature scaling completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training and Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = {}\n",
        "predictions = {}\n",
        "probabilities = {}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training {name}...\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    # Use scaled data for Logistic Regression, original for tree-based models\n",
        "    if name == 'Logistic Regression':\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_proba)\n",
        "    \n",
        "    results[name] = {\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1,\n",
        "        'ROC-AUC': roc_auc\n",
        "    }\n",
        "    \n",
        "    predictions[name] = y_pred\n",
        "    probabilities[name] = y_proba\n",
        "    \n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison DataFrame\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df = results_df.round(4)\n",
        "print(\"\\nModel Performance Comparison:\")\n",
        "print(\"=\"*60)\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize metrics comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Bar plot for all metrics\n",
        "results_df.plot(kind='bar', ax=axes[0], width=0.8)\n",
        "axes[0].set_title('Model Performance Metrics Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Model', fontsize=12)\n",
        "axes[0].set_ylabel('Score', fontsize=12)\n",
        "axes[0].legend(loc='best')\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# ROC-AUC comparison\n",
        "results_df['ROC-AUC'].plot(kind='bar', ax=axes[1], color='steelblue', width=0.6)\n",
        "axes[1].set_title('ROC-AUC Score Comparison', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Model', fontsize=12)\n",
        "axes[1].set_ylabel('ROC-AUC Score', fontsize=12)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "axes[1].axhline(y=0.5, color='r', linestyle='--', label='Random Classifier')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. ROC Curves\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot ROC curves for all models\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for name in models.keys():\n",
        "    fpr, tpr, _ = roc_curve(y_test, probabilities[name])\n",
        "    auc_score = roc_auc_score(y_test, probabilities[name])\n",
        "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.4f})', linewidth=2)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.5000)', linewidth=1)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curves Comparison', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Confusion Matrices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrices for all models\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for idx, name in enumerate(models.keys()):\n",
        "    cm = confusion_matrix(y_test, predictions[name])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
        "                cbar_kws={'shrink': 0.8})\n",
        "    axes[idx].set_title(f'{name}\\nAccuracy: {results[name][\"Accuracy\"]:.4f}', \n",
        "                       fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Predicted', fontsize=10)\n",
        "    axes[idx].set_ylabel('Actual', fontsize=10)\n",
        "    axes[idx].set_xticklabels(['Not Creditworthy', 'Creditworthy'])\n",
        "    axes[idx].set_yticklabels(['Not Creditworthy', 'Creditworthy'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Feature Importance (Tree-based Models)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance for Decision Tree and Random Forest\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "for idx, name in enumerate(['Decision Tree', 'Random Forest']):\n",
        "    model = models[name]\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'importance': model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    # Plot top 10 features\n",
        "    top_features = feature_importance.head(10)\n",
        "    axes[idx].barh(range(len(top_features)), top_features['importance'], color='steelblue')\n",
        "    axes[idx].set_yticks(range(len(top_features)))\n",
        "    axes[idx].set_yticklabels(top_features['feature'])\n",
        "    axes[idx].set_xlabel('Importance', fontsize=12)\n",
        "    axes[idx].set_title(f'Top 10 Feature Importance - {name}', fontsize=12, fontweight='bold')\n",
        "    axes[idx].grid(axis='x', alpha=0.3)\n",
        "    axes[idx].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Cross-Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform cross-validation for more robust evaluation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    if name == 'Logistic Regression':\n",
        "        X_data = X_train_scaled\n",
        "    else:\n",
        "        X_data = X_train\n",
        "    \n",
        "    cv_scores = cross_val_score(model, X_data, y_train, cv=cv, scoring='roc_auc')\n",
        "    cv_results[name] = {\n",
        "        'Mean ROC-AUC': cv_scores.mean(),\n",
        "        'Std ROC-AUC': cv_scores.std(),\n",
        "        'Scores': cv_scores\n",
        "    }\n",
        "    \n",
        "    print(f\"{name} Cross-Validation ROC-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "\n",
        "# Visualize cross-validation results\n",
        "cv_df = pd.DataFrame({\n",
        "    name: cv_results[name]['Scores'] for name in models.keys()\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "cv_df.boxplot()\n",
        "plt.title('Cross-Validation ROC-AUC Scores', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('ROC-AUC Score', fontsize=12)\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Summary and Conclusions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"FINAL MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nTest Set Performance:\")\n",
        "print(results_df)\n",
        "\n",
        "print(\"\\n\\nCross-Validation Performance:\")\n",
        "cv_summary = pd.DataFrame({\n",
        "    name: [cv_results[name]['Mean ROC-AUC'], cv_results[name]['Std ROC-AUC']]\n",
        "    for name in models.keys()\n",
        "}, index=['Mean ROC-AUC', 'Std ROC-AUC']).T\n",
        "print(cv_summary)\n",
        "\n",
        "print(\"\\n\\nBest Model:\")\n",
        "best_model = results_df['ROC-AUC'].idxmax()\n",
        "print(f\"Based on ROC-AUC: {best_model} ({results_df.loc[best_model, 'ROC-AUC']:.4f})\")\n",
        "\n",
        "best_f1 = results_df['F1-Score'].idxmax()\n",
        "print(f\"Based on F1-Score: {best_f1} ({results_df.loc[best_f1, 'F1-Score']:.4f})\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
